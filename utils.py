import cv2, yt_dlp, os, time, librosa, tempfile, uuid, glob, time, torch, re
from panns_inference import AudioTagging, SoundEventDetection, labels
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import soundfile as sf
from tqdm import tqdm
import torch.nn.functional as F
import re

# * ------------------ ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì¶œ ------------------

"""
def openVideoStream(youtube_url: str) -> cv2.VideoCapture:
    
    # OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube ë¹„ë””ì˜¤ì˜ ìŠ¤íŠ¸ë¦¼ì„ ì—¬ëŠ” í•¨ìˆ˜
    # @param video_id: YouTube ì˜ìƒì˜ ID
    # @return: ì—´ê¸° ì‹¤íŒ¨ ì‹œ False, ì„±ê³µì‹œ cv2.VideoCapture ê°ì²´
    
    print(f"'{youtube_url}'ì—ì„œ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ URLì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

    video_stream_url = None
    ydl_options = {
        "format": "bestvideo/best",
        "noplaylist": True,
        "quiet": True,
        "no_warnings": True,
    }

    with yt_dlp.YoutubeDL(ydl_options) as ydl:
        info_dict = ydl.extract_info(youtube_url, download=False)

        if "formats" in info_dict:
            for f in info_dict["formats"]:
                if f.get("vcodec") != "none" and f.get("url"):
                    if (
                        "bestvideo" in f.get("format_note", "").lower()
                        or "video only" in f.get("format_note", "").lower()
                    ):
                        video_stream_url = f["url"]
                        break

        if not video_stream_url:
            for f in info_dict["formats"]:
                if (
                    f.get("url")
                    and f.get("acodec") != "none"
                    and f.get("vcodec") != "none"
                ):
                    video_stream_url = f["url"]
                    break

        if not video_stream_url:
            print(
                f"ì˜¤ë¥˜: '{youtube_url}'ì—ì„œ ìœ íš¨í•œ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
            print(f"yt-dlp ì •ë³´: {info_dict}")
            return False

    cap = cv2.VideoCapture(video_stream_url)
    if not cap.isOpened():
        print(
            f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. URLì´ ìœ íš¨í•œì§€, ì½”ë± ë¬¸ì œê°€ ì•„ë‹Œì§€ í™•ì¸í•˜ì„¸ìš”: {video_stream_url}"
        )
        return False

    print("ìŠ¤íŠ¸ë¦¼ì´ ì—´ë ¸ìŠµë‹ˆë‹¤.")
    return cap
"""


"""
def openVideoStream(youtube_url: str) -> cv2.VideoCapture:
    
    # OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube ë¹„ë””ì˜¤ì˜ ìŠ¤íŠ¸ë¦¼(30fps)ì„ ì—¬ëŠ” í•¨ìˆ˜, 30fps ì‹¤íŒ¨ ì‹œ ìµœëŒ€ fpsë¡œ\n
    # ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœê³  í™”ì§ˆì„ ê°€ì ¸ì˜´\n
    # ê°„í˜¹ ì¿ í‚¤ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ\n
    # getcookies.txt êµ¬ê¸€ í”ŒëŸ¬ê·¸ì¸ ì„¤ì¹˜ í›„, ìœ íŠœë¸Œë¡œ ë“¤ì–´ê°€ ì¿ í‚¤ íŒŒì¼ export í›„ cookiefile ê²½ë¡œì— ì €ì¥í•˜ë©´ í•´ê²° ê°€ëŠ¥\n
    # êµ¬ê¸€ ê³„ì •ì´ ì—¬ëŸ¬ê°€ì§€ ë¡œê·¸ì¸ì´ ë˜ì–´ìˆë‹¤ë©´ ë°”ê¿”ê°€ë©´ì„œ ë‹¤ ë„£ì–´ë³´ê¸°\n
    # ë³´ì•ˆìƒ ì¿ í‚¤ íŒŒì¼ì€ pushí•˜ì§€ ì•Šì•˜ìŒ

    # @param youtube_url: YouTube ë¹„ë””ì˜¤ì˜ URL
    # @return: ì—´ê¸° ì‹¤íŒ¨ ì‹œ False, ì„±ê³µì‹œ cv2.VideoCapture ê°ì²´
    
    print(f"'{youtube_url}'ì—ì„œ 30fps ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ URLì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

    ydl_options = {
        "format": "bestvideo+bestaudio/best",
        "noplaylist": True,
        "quiet": True,
        "no_warnings": True,
        "cookiefile": "./cooks.txt",
    }

    with yt_dlp.YoutubeDL(ydl_options) as ydl:
        info_dict = ydl.extract_info(youtube_url, download=False)
        formats = info_dict.get("formats", [])

        # 1ì°¨ ì‹œë„: 30fps + video-only + ìµœëŒ€ í•´ìƒë„
        best_format = None
        max_height = -1
        for f in formats:
            if (
                f.get("vcodec") != "none"
                and f.get("acodec") == "none"
                and f.get("url")
                and f.get("fps") == 30
                and f.get("height", 0) > max_height
            ):
                best_format = f
                max_height = f["height"]

        # 2ì°¨ ì‹œë„: fps ë¬´ì‹œí•˜ê³  video-only ì¤‘ ìµœëŒ€ í•´ìƒë„
        if not best_format:
            print("âš ï¸ 30fps ìŠ¤íŠ¸ë¦¼ ì—†ìŒ â†’ ìµœëŒ€ í™”ì§ˆë¡œ fallback")
            max_height = -1
            for f in formats:
                if (
                    f.get("vcodec") != "none"
                    and f.get("acodec") == "none"
                    and f.get("url")
                    and f.get("height", 0) > max_height
                ):
                    best_format = f
                    max_height = f["height"]

        if not best_format:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ video-only ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        video_stream_url = best_format["url"]
        print(
            f"ğŸ¥ ì„ íƒëœ í•´ìƒë„: {best_format['height']}p @ {best_format.get('fps', 'N/A')}fps\nURL: {video_stream_url}"
        )

        cap = cv2.VideoCapture(video_stream_url)
        if not cap.isOpened():
            print("âŒ OpenCVë¡œ ìŠ¤íŠ¸ë¦¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        return cap
"""


def openVideoStream(youtube_url: str) -> cv2.VideoCapture:
    """
    OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ YouTube progressive ìŠ¤íŠ¸ë¦¼ ì¤‘ 720p + 30fpsë¥¼ ìš°ì„  ì„ íƒí•˜ê³ ,
    720pê°€ ì—†ì„ ê²½ìš° progressive ì¤‘ ìµœëŒ€ í•´ìƒë„ ìŠ¤íŠ¸ë¦¼ì„ fallbackìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
    """
    print(f"'{youtube_url}'ì—ì„œ progressive 720p ìŠ¤íŠ¸ë¦¼ URLì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

    ydl_options = {
        "format": "bestvideo+bestaudio/best",
        "noplaylist": True,
        "quiet": True,
        "no_warnings": True,
        "cookiefile": "./cooks.txt",  # í•„ìš” ì—†ë‹¤ë©´ ìƒëµ ê°€ëŠ¥
    }

    with yt_dlp.YoutubeDL(ydl_options) as ydl:
        info_dict = ydl.extract_info(youtube_url, download=False)
        formats = info_dict.get("formats", [])

        # âœ… 1ì°¨ ì‹œë„: progressive + 30fps + height == 720
        selected_format = None
        for f in formats:
            if (
                f.get("vcodec") != "none"
                and f.get("acodec") != "none"
                and f.get("url")
                and f.get("height") == 720
                and f.get("fps") == 30
            ):
                selected_format = f
                break

        # ğŸ” 2ì°¨ ì‹œë„: progressive ì¤‘ ìµœëŒ€ í•´ìƒë„
        if not selected_format:
            print(
                "âš ï¸ 720p @30fps progressive ìŠ¤íŠ¸ë¦¼ ì—†ìŒ â†’ fallbackìœ¼ë¡œ ìµœëŒ€ í•´ìƒë„ progressive ì„ íƒ"
            )
            max_height = -1
            for f in formats:
                if (
                    f.get("vcodec") != "none"
                    and f.get("acodec") != "none"
                    and f.get("url")
                    and f.get("height", 0) > max_height
                ):
                    selected_format = f
                    max_height = f["height"]

        if not selected_format:
            print("âŒ progressive ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        video_stream_url = selected_format["url"]
        print(
            f"ğŸ¬ ì„ íƒëœ ìŠ¤íŠ¸ë¦¼: {selected_format['ext']} | "
            f"{selected_format['height']}p @ {selected_format.get('fps', 'N/A')}fps"
        )
        print(f"URL: {video_stream_url}")
        cap = cv2.VideoCapture(video_stream_url)
        if not cap.isOpened():
            print("âŒ OpenCVë¡œ ìŠ¤íŠ¸ë¦¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        return cap


"""
def extractFrames(cap, start_time_sec, duration_sec, fps, output_folder_path):
    
    # ì—°ì†ì ìœ¼ë¡œ í”„ë ˆì„ì„ ì½ìœ¼ë©° ì¼ì • ê°„ê²©ë§ˆë‹¤ ì €ì¥
    # duration_secì´ Noneì´ë©´ start_time_secë¶€í„° ì˜ìƒ ëê¹Œì§€ ì¶”ì¶œ

    # @param cap: cv2.VideoCapture ê°ì²´
    # @param start_time_sec: ì¶”ì¶œì„ ì‹œì‘í•  ì˜ìƒ ì§€ì (ì´ˆ)
    # @param duration_sec: ì¶”ì¶œí•  ê¸¸ì´(ì´ˆ)
    # @param fps: ì´ˆë‹¹ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜
    # @param output_folder_path: ì´ë¯¸ì§€ ì €ì¥ í´ë”
    

    if not cap.isOpened():
        print("âŒ VideoCapture ì—´ê¸° ì‹¤íŒ¨")
        return False

    os.makedirs(output_folder_path, exist_ok=True)

    # ì˜ìƒ ê¸°ë³¸ ì •ë³´
    video_fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total_duration_sec = total_frames / video_fps

    # ì‹œì‘/ì¢…ë£Œ í”„ë ˆì„ ê³„ì‚°
    start_frame = int(start_time_sec * video_fps)
    if duration_sec is None:
        end_frame = total_frames
        print(f"ğŸ“Œ duration_sec=None â†’ ëê¹Œì§€ ì¶”ì¶œ (ì´ {total_duration_sec:.2f}s)")
    else:
        end_frame = int((start_time_sec + duration_sec) * video_fps)

    frame_interval = max(1, int(round(video_fps / fps)))

    print(f"\nğŸ¬ ì‹œì‘ í”„ë ˆì„: {start_frame}, ì¢…ë£Œ í”„ë ˆì„: {end_frame}")
    print(f"ğŸ¯ í”„ë ˆì„ ê°„ê²©: {frame_interval} (video fps: {video_fps:.2f})")

    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    extraction_start_time = time.time()
    current_frame = start_frame
    saved_count = 0

    while current_frame < end_frame:
        ret, frame = cap.read()
        if not ret:
            print("âŒ í”„ë ˆì„ ì½ê¸° ì‹¤íŒ¨ ë˜ëŠ” ì˜ìƒ ë ë„ë‹¬")
            break

        if (current_frame - start_frame) % frame_interval == 0:
            filename = os.path.join(
                output_folder_path,
                (
                    f"{start_time_sec}s_{fps}fps_{saved_count:03d}.jpg"
                    if duration_sec is None
                    else f"{start_time_sec}s_{duration_sec}s_{fps}fps_{saved_count:03d}.jpg"
                ),
            )
            cv2.imwrite(filename, frame, [cv2.IMWRITE_JPEG_QUALITY, 30])
            saved_count += 1

        current_frame += 1

    extraction_end_time = time.time()
    print(
        f"âœ… ì´ {saved_count}ê°œ í”„ë ˆì„ ì €ì¥ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {extraction_end_time - extraction_start_time:.2f}ì´ˆ"
    )
    return saved_count
"""


def extractFrames(
    cap,
    start_time_sec,
    duration_sec,
    fps,
    output_folder_path,
    resize_to=(320, 180),  # ì›í•˜ëŠ” í•´ìƒë„ (width, height)
):
    """
    duration_secì´ Noneì´ë©´ start_time_secë¶€í„° ëê¹Œì§€ ì¶”ì¶œ\n
    fpsëŠ” cap.get(cv2.CAP_PROP_FPS) ê¶Œì¥ (ì›ë³¸ ì˜ìƒì˜ fps)\n
    grab + retrieve + resize ìµœì í™”ëœ í”„ë ˆì„ ì¶”ì¶œ
    """

    if not cap.isOpened():
        print("âŒ VideoCapture ì—´ê¸° ì‹¤íŒ¨")
        return False

    os.makedirs(output_folder_path, exist_ok=True)

    # ì˜ìƒ ì •ë³´
    video_fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    total_duration_sec = total_frames / video_fps

    # ì‹œì‘/ì¢…ë£Œ í”„ë ˆì„ ê³„ì‚°
    start_frame = int(start_time_sec * video_fps)
    if duration_sec is None:
        end_frame = total_frames
        print(f"ğŸ“Œ duration_sec=None â†’ ëê¹Œì§€ ì¶”ì¶œ (ì´ {total_duration_sec:.2f}s)")
    else:
        end_frame = int((start_time_sec + duration_sec) * video_fps)

    frame_interval = max(1, int(round(video_fps / fps)))

    print(f"\nğŸ¬ ì‹œì‘ í”„ë ˆì„: {start_frame}, ì¢…ë£Œ í”„ë ˆì„: {end_frame}")
    print(f"ğŸ¯ í”„ë ˆì„ ê°„ê²©: {frame_interval} (video fps: {video_fps:.2f})")
    print(f"ğŸ–¼ï¸ í•´ìƒë„ ì¶•ì†Œ: {resize_to[0]}x{resize_to[1]}")

    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

    extraction_start_time = time.time()
    current_frame = start_frame
    saved_count = 0

    while current_frame < end_frame:
        grabbed = cap.grab()
        if not grabbed:
            print("âŒ grab ì‹¤íŒ¨ ë˜ëŠ” ì˜ìƒ ë ë„ë‹¬")
            break

        if (current_frame - start_frame) % frame_interval == 0:
            ret, frame = cap.retrieve()
            if not ret or frame is None:
                print("âŒ retrieve ì‹¤íŒ¨")
                break

            # í•´ìƒë„ ì¤„ì´ê¸°
            frame = cv2.resize(frame, resize_to, interpolation=cv2.INTER_AREA)

            filename = os.path.join(
                output_folder_path,
                (
                    f"{start_time_sec}s_{fps}fps_{saved_count:03d}.jpg"
                    if duration_sec is None
                    else f"{start_time_sec}s_{duration_sec}s_{fps}fps_{saved_count:03d}.jpg"
                ),
            )
            cv2.imwrite(filename, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
            saved_count += 1

        current_frame += 1

    extraction_end_time = time.time()
    print(
        f"âœ… ì´ {saved_count}ê°œ í”„ë ˆì„ ì €ì¥ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {extraction_end_time - extraction_start_time:.2f}ì´ˆ"
    )
    return saved_count


def extractFrames720p(
    cap: cv2.VideoCapture,
    start_time_sec: int,
    duration_sec: int,
    fps: int,
    output_folder_path: str,
):
    """
    ì—´ë ¤ ìˆëŠ” VideoCapture ìŠ¤íŠ¸ë¦¼ì—ì„œ íŠ¹ì • êµ¬ê°„ì˜ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ê³  ì €ì¥\n
    720pë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ì—¬ ì €ì¥í•¨

    @param cap: cv2.VideoCapture ê°ì²´
    @param start_time_sec: ì¶”ì¶œì„ ì‹œì‘í•  ì˜ìƒ ì§€ì (ì´ˆ)
    @param duration_sec: ì¶”ì¶œí•  ê¸¸ì´(ì´ˆ)
    @param fps: ì´ˆë‹¹ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜
    @param output_folder_path: ì´ë¯¸ì§€ ì €ì¥ í´ë”
    """
    if not cap.isOpened():
        print("ì˜¤ë¥˜: ìœ íš¨í•˜ì§€ ì•Šì€ VideoCapture ê°ì²´")
        return False

    os.makedirs(output_folder_path, exist_ok=True)
    print(
        f"\n{start_time_sec}ì´ˆë¶€í„° {duration_sec}ì´ˆ ë™ì•ˆ ì´ˆë‹¹ {fps} í”„ë ˆì„ ì¶”ì¶œ ì‹œì‘..."
    )

    frame_count = 0
    last_saved_msec = start_time_sec * 1000
    target_frame_interval_msec = 1000 / fps

    cap.set(cv2.CAP_PROP_POS_MSEC, start_time_sec * 1000)
    actual_start_msec = cap.get(cv2.CAP_PROP_POS_MSEC)
    if abs(actual_start_msec - (start_time_sec * 1000)) > 1000:
        print(f"ê²½ê³ : íƒìƒ‰ ì •í™•ë„ ë‚®ìŒ (ì‹œì‘ ìœ„ì¹˜ {actual_start_msec/1000:.2f}s)")

    extraction_start_time = time.time()

    while True:
        current_msec = cap.get(cv2.CAP_PROP_POS_MSEC)

        if current_msec / 1000.0 >= (start_time_sec + duration_sec):
            print(f"ğŸ›‘ {start_time_sec}-{start_time_sec + duration_sec}ì´ˆ ì¶”ì¶œ ì™„ë£Œ.")
            break

        ret, frame = cap.read()
        if not ret:
            print("âŒ ë” ì´ìƒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŒ")
            break

        if (
            frame_count == 0
            or (current_msec - last_saved_msec) >= target_frame_interval_msec
        ):
            # âœ… 720pë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            resized_frame = cv2.resize(frame, (1280, 720), interpolation=cv2.INTER_AREA)

            filename = os.path.join(
                output_folder_path,
                f"{start_time_sec}s_{duration_sec}s_{fps}fps_{frame_count:03d}.jpg",
            )
            cv2.imwrite(filename, resized_frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
            frame_count += 1
            last_saved_msec = current_msec

    extraction_end_time = time.time()
    print(
        f"âœ… ì´ {frame_count}ê°œ í”„ë ˆì„ ì €ì¥ ì™„ë£Œ. ì†Œìš” ì‹œê°„: {extraction_end_time - extraction_start_time:.2f}ì´ˆ"
    )
    return frame_count


# * ------------------- Audio recognition -------------------


def getAudioCropped(autio_path, start_sec=None, end_sec=None):
    """
    í˜„ì¬ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ë¶ˆëŸ¬ì™€ (1, size)ë¡œ ë°˜í™˜

    @param autio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    @param start_sec: cropí•  ì‹œì‘ì , Noneì´ë©´ ì „ì²´
    @param end_sec: cropí•  ëì , Noneì´ë©´ ì „ì²´
    @return: (1, size) shaped numpy ì˜¤ë””ì˜¤, ìƒ˜í”Œë ˆì´íŠ¸
    """
    audio, sr = librosa.load(autio_path, sr=32000, mono=True)

    if start_sec == None and end_sec == None:
        return audio[None, :], sr

    start_sample, end_sample = int(start_sec * sr), int(end_sec * sr)  # type: ignore
    cropped = audio[start_sample:end_sample]
    return cropped[None, :], sr


def getAudioCroppedFromURL(
    youtube_url, start_sec=None, end_sec=None, isMono=True, sr=32000
):
    """
    YouTube URLì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ë¶ˆëŸ¬ì™€ (1, size)ë¡œ ë°˜í™˜
    tmp ë””ë ‰í† ë¦¬ì— íŒŒì¼ì´ ì €ì¥ë˜ì–´ í•¨ìˆ˜ ì‹¤í–‰ í›„ ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ì€ ì‚­ì œë¨
    ë¡œì»¬ì— ffmpeg ì„¤ì¹˜ ë° í™˜ê²½ë³€ìˆ˜ ì„¸íŒ…ì´ í•„ìš”

    @param youtube_url: youtube URL
    @param start_sec: cropí•  ì‹œì‘ì , Noneì´ë©´ ì „ì²´
    @param end_sec: cropí•  ëì , Noneì´ë©´ ì „ì²´
    @param isMono: monoë¡œ ê°€ì ¸ì˜¬ì§€, steroë¡œ ê°€ì ¸ì˜¬ì§€, default=True
    @param sr: default=32,000 (PANNs ì…ë ¥ì„ ìœ„í•´)
    @return: (1, size) shaped numpy ì˜¤ë””ì˜¤, ìƒ˜í”Œë ˆì´íŠ¸
    """
    tmp_dir = tempfile.gettempdir()
    tmp_id = uuid.uuid4().hex
    tmp_basename = os.path.join(tmp_dir, tmp_id)

    try:
        ydl_opts = {
            "format": "bestaudio/best",
            "outtmpl": f"{tmp_basename}.%(ext)s",
            "quiet": True,
            "no_warnings": True,
            "cookiefile": "./cooks.txt",  #
            "postprocessors": [
                {
                    "key": "FFmpegExtractAudio",
                    "preferredcodec": "wav",
                    "preferredquality": "128",
                }
            ],
        }

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(youtube_url, download=True)
            wav_path = f"{tmp_basename}.wav"

        # librosa ë¡œë“œ, PANNs ëª¨ë¸ì€ 32kHz ê¸°ì¤€
        audio, sr = librosa.load(wav_path, sr=sr, mono=isMono)

        if start_sec is None and end_sec is None:
            return audio[None, :], sr

        start_sample = int(start_sec * sr)
        end_sample = int(end_sec * sr)
        cropped = audio[start_sample:end_sample]
        return cropped[None, :], sr

    finally:
        # ë‹¤ìš´ë¡œë“œí•œ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        for ext in ["wav", "webm", "m4a"]:
            f = f"{tmp_basename}.{ext}"
            if os.path.exists(f):
                os.remove(f)


def audioTagging(device: str, audio, n=10):
    """
    ì „ì²´ ì˜ìƒì—ì„œ ë“±ì¥í•˜ëŠ” ì˜¤ë””ì˜¤ ìƒìœ„ nê°œ ë ˆì´ë¸”ê³¼ í™•ë¥  ì¶œë ¥

    @param device: 'cpu' ë˜ëŠ” 'cuda'
    @param audio: ì˜¤ë””ì˜¤ ë°ì´í„° (1, N) numpy array
    @param n: ì¶œë ¥í•  ìƒìœ„ í´ë˜ìŠ¤ ìˆ˜ (ìµœëŒ€ 527)
    @return: clipwise_output (1, 527) í™•ë¥  ë²¡í„°
    """
    at = AudioTagging(checkpoint_path=None, device=device)
    clipwise_output, _ = at.inference(audio)
    top_indices = np.argsort(clipwise_output[0])[::-1][:n]

    for i in top_indices:
        print(f"{labels[i]:30s} : {clipwise_output[0][i]:.3f}")

    return clipwise_output


def eventDetectionWithOverallTopk(device: str, audio, clipwise_output, n=5):
    """
    ì „ì²´ ì˜ìƒì—ì„œ ë“±ì¥í•˜ëŠ” ì˜¤ë””ì˜¤ ìƒìœ„ nê°œ í´ë˜ìŠ¤ì˜ í”„ë ˆì„ë³„ í™•ë¥  ì‹œê°í™”

    @param device: 'cpu' ë˜ëŠ” 'cuda'
    @param audio: ì˜¤ë””ì˜¤ ë°ì´í„° (1, N) numpy array
    @param clipwise_output: audioTagging ê²°ê³¼ë¡œ ë‚˜ì˜¨ í™•ë¥  ë²¡í„°
    @param n: ì‹œê°í™”í•  í´ë˜ìŠ¤ ìˆ˜
    @return: ì—†ìŒ (ê·¸ë˜í”„ ì¶œë ¥)
    """
    sed = SoundEventDetection(checkpoint_path=None, device=device)
    framewise_output = sed.inference(audio)[0]

    top_indices = np.argsort(clipwise_output[0])[::-1][:n]

    plt.figure(figsize=(12, 6))

    for i in top_indices:
        plt.plot(framewise_output[:, i], label=labels[i])

    plt.title(f"Top-{n} predicted sound classes over frames")
    plt.xlabel("Frame Index")
    plt.ylabel("Probability")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()


def eventDetectionWithPerTopk(device: str, audio, min_score=0.2):
    """
    ê° í”„ë ˆì„ì— ëŒ€í•´, min_score ì´ìƒ ì˜¤ë””ì˜¤ í´ë˜ìŠ¤ ì‹œê°í™”

    @param device: 'cpu' or 'cuda'
    @param audio: (1, N) shaped numpy audio
    @param min_score: ì‹œê°í™” ëŒ€ìƒ í´ë˜ìŠ¤ì˜ ìµœì†Œ ìµœëŒ€ í™•ë¥  ê¸°ì¤€
    """
    sr = 32000
    sed = SoundEventDetection(checkpoint_path=None, device=device)
    framewise_output = sed.inference(audio)[0]  # shape: (T, 527)

    # â±ï¸ ì‹œê°„ì¶• (ì´ˆ ë‹¨ìœ„)
    num_frames = framewise_output.shape[0]
    duration_sec = audio.shape[1] / sr
    x = np.linspace(0, duration_sec, num_frames)

    # ğŸ¯ í´ë˜ìŠ¤ë³„ ìµœëŒ€ í™•ë¥  ê³„ì‚°
    max_scores = np.max(framewise_output, axis=0)
    filtered_indices = np.where(max_scores >= min_score)[0]

    # ğŸ“‹ ì¶œë ¥ + ì‹œê°í™”
    if len(filtered_indices) == 0:
        print(f"\nâ— ìµœëŒ€ í™•ë¥ ì´ {min_score} ì´ìƒì¸ í´ë˜ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"\nìµœëŒ€ í™•ë¥ ì´ {min_score} ì´ìƒì¸ í´ë˜ìŠ¤ ìˆ˜: {len(filtered_indices)}ê°œ")
    for idx in filtered_indices[np.argsort(max_scores[filtered_indices])[::-1]]:
        label = labels[idx]
        max_val = max_scores[idx]
        print(f"- {label}: ìµœëŒ€ {max_val:.3f}")

    # ğŸ“ˆ ì‹œê°í™”
    plt.figure(figsize=(15, 6))
    for idx in filtered_indices:
        plt.plot(x, framewise_output[:, idx], label=labels[idx])
    plt.title(f"Classes with max score â‰¥ {min_score}")
    plt.xlabel("Time (seconds)")
    plt.ylabel("Probability")
    plt.legend()
    plt.grid(True)
    plt.xticks(np.arange(0, int(duration_sec) + 1, 1))
    plt.tight_layout()
    plt.show()


# ì˜¤ë””ì˜¤ì— ëŒ€í•´ ìŠ¤í”¼ì¹˜ - ìŒì•… ë¶„ë¦¬
def split_audio_to_speech_music(audio, model):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ìŠ¤í”¼ì¹˜ì™€ ìŒì•…ìœ¼ë¡œ ë¶„ë¦¬

    @param audio: ì˜¤ë””ì˜¤ numpy ë°°ì—´, getAudioCroppedFromURL í•¨ìˆ˜ì—ì„œ isMono = Falseë¡œ ì§€ì •í•˜ê³  ê°€ì ¸ì™€ì•¼í•¨
    @param model: ëª¨ë¸, get_model(name='htdemucs').to(device)
    @return: sources, sources[0]: drums, sources[1]: bass, sources[2]: other, sources[3]: vocal\n
            Steroì´ë¯€ë¡œ (2, N) í˜•íƒœì„
    """
    import torch
    from demucs.apply import apply_model

    is_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if is_cuda else "cpu")

    audio_tensor = torch.tensor(audio, dtype=torch.float32).to(device)

    model.eval()

    with torch.no_grad():
        sources = apply_model(model, audio_tensor, device=device.type, split=True)

    sources = sources.squeeze(0).cpu()

    return sources


# * ------------------- Audio segmentation & ì¡°ìŒì†ë„ ê³„ì‚° -------------------


# 1. ì˜¤ë””ì˜¤ ìë¥´ê¸°
def split_audio(audio_path, start_time, end_time, chunk_size=1, sr=16000):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì§€ì •ëœ ì‹œê°„ êµ¬ê°„ìœ¼ë¡œ ì²­í¬ ë¶„í•  í›„ ì €ì¥

    @param audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    @param start_time: ë¶„í•  ì‹œì‘ ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
    @param end_time: ë¶„í•  ë ì‹œê°„ (ì´ˆ ë‹¨ìœ„)
    @param chunk_size: ê° ì²­í¬ì˜ í¬ê¸° (ì´ˆ ë‹¨ìœ„)
    @param sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 16000)
    """
    audio, _ = librosa.load(audio_path, sr=sr)
    chunks = []
    print("ì²­í¬ ë¶„í•  ì§„í–‰")
    for t in range(start_time, end_time, chunk_size):
        start_sample = int(t * sr)
        end_sample = int(min((t + chunk_size), end_time) * sr)
        chunk = audio[start_sample:end_sample]

        chunk_path = f"./chunk/chunk_{t}_{t+chunk_size}.wav"
        sf.write(chunk_path, chunk, sr)
        chunks.append((t, t + chunk_size, chunk_path))
    return chunks


# 2. ì¹¨ë¬µ ì œì™¸ ë°œí™” ì‹œê°„ ê³„ì‚°
def get_speech_duration(audio_path, top_db):
    y, sr = librosa.load(audio_path, sr=None)
    intervals = librosa.effects.split(y, top_db=top_db)
    speech_duration = sum((end - start) for start, end in intervals) / sr
    return speech_duration


# 3. fast-whisperë¡œ ìë§‰ ì¶”ì¶œ + ì¡°ìŒì†ë„ ê³„ì‚°
def estimate_articulation_rate_fast_whisper(chunks, model, top_db=30):
    results = []
    print("ì¡°ìŒ ì†ë„ ê³„ì‚° ì§„í–‰ì¤‘...")

    for start, end, chunk_path in tqdm(chunks, desc="Processing chunks"):
        try:
            segments, _ = model.transcribe(chunk_path, language="ko", beam_size=5)
            text = "".join([seg.text.replace(" ", "") for seg in segments])
            text = re.sub(r"[.,!?\"'â€œâ€â€˜â€™â€¦\-â€“â€”():;]", "", text)
            num_chars = len(text)

            speech_dur = get_speech_duration(chunk_path, top_db)
            articulation_rate = num_chars / speech_dur if speech_dur > 0 else 0

            results.append(
                {
                    "start": start,
                    "end": end,
                    "text": text,
                    "chars": num_chars,
                    "speech_duration": round(speech_dur, 3),
                    "articulation_rate": round(articulation_rate, 2),
                }
            )
        except Exception as e:
            results.append(
                {
                    "start": start,
                    "end": end,
                    "text": "",
                    "chars": 0,
                    "speech_duration": 0,
                    "articulation_rate": 0,
                    "error": str(e),
                }
            )
    return results


# * ìƒ· ê°ì§€


def compute_hist_diff(frame1, frame2, bins=16):
    # RGB Color ì‚¬ìš©
    hist1 = cv2.calcHist([frame1], [0, 1, 2], None, [bins] * 3, [0, 256] * 3)
    hist2 = cv2.calcHist([frame2], [0, 1, 2], None, [bins] * 3, [0, 256] * 3)

    # hist ì •ê·œí™”
    hist1 = cv2.normalize(hist1, hist1, 0, 1, cv2.NORM_MINMAX).flatten()
    hist2 = cv2.normalize(hist2, hist2, 0, 1, cv2.NORM_MINMAX).flatten()

    # hist ë¹„êµ
    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_BHATTACHARYYA)


def shot_boundary_detection(
    video_source,  # íŒŒì¼ ê²½ë¡œ, ìŠ¤íŠ¸ë¦¼ URL, ë˜ëŠ” ì´ë¯¸ ì—´ë¦° VideoCapture ê°ì²´
    scale_factor=2.0,
    bins=16,
    window_size=30,
    resize_dim=None,
):
    """
    ë¹„ë””ì˜¤ì—ì„œ ìƒ· ê²½ê³„ë¥¼ ê°ì§€í•˜ëŠ” í•¨ìˆ˜ (ìƒ‰ íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ ê¸°ë°˜)

    :param video_source: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ(str), ìŠ¤íŠ¸ë¦¼ URL(str), ë˜ëŠ” cv2.VideoCapture ê°ì²´
    :param scale_factor: ìƒ· ê²½ê³„ ì„ê³„ê°’ì„ ì„¤ì •í•˜ê¸° ìœ„í•œ í‰ê·  ì°¨ì´ì— ëŒ€í•œ ìŠ¤ì¼€ì¼ íŒ©í„°, ë†’ì„ìˆ˜ë¡ í™•ì‹¤í•œ ìƒ· ê²½ê³„ë§Œ ê°ì§€
    :param bins: íˆìŠ¤í† ê·¸ë¨ ë¹ˆ ê°œìˆ˜ (ë‚®ì„ìˆ˜ë¡ ë¹ ë¥´ê³ , ë„ˆë¬´ ë‚®ìœ¼ë©´ ì •í™•ë„ ì €í•˜)
    :param window_size: ì´ë™ í‰ê·  ë° í‘œì¤€ í¸ì°¨ ê³„ì‚°ì„ ìœ„í•œ ìœˆë„ìš° í¬ê¸° (í”„ë ˆì„ ìˆ˜)\n
                        ì´ì „ Nê°œ í”„ë ˆì„ì˜ ì°¨ì´ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ê³„ê°’ì„ ë™ì ìœ¼ë¡œ ì„¤ì •
    :param resize_dim: ë¦¬ì‚¬ì´ì¦ˆí•  (width, height). Noneì´ë©´ ì›ë³¸\n
                        ì˜ˆ: (640, 360) ë˜ëŠ” (320, 180). (640, 360)ì´ë©´ ì¶©ë¶„í•œ ê²ƒìœ¼ë¡œ ë³´ì„
    :return: ìƒ· ê²½ê³„ í”„ë ˆì„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸, íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ ë¦¬ìŠ¤íŠ¸
    """
    # video_sourceê°€ ë¬¸ìì—´ì´ë©´ ìƒˆë¡œ VideoCapture ìƒì„±
    if isinstance(video_source, str):
        cap = cv2.VideoCapture(video_source)
        video_label = video_source
    elif isinstance(video_source, cv2.VideoCapture):
        cap = video_source
        video_label = "VideoCapture Stream"
    else:
        raise ValueError("video_sourceëŠ” str ë˜ëŠ” cv2.VideoCaptureì—¬ì•¼ í•©ë‹ˆë‹¤.")

    if not cap.isOpened():
        print(f"Cannot open video: {video_label}")
        return [], []

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames <= 0:
        print("ê²½ê³ : ìŠ¤íŠ¸ë¦¼ì—ì„œëŠ” ì´ í”„ë ˆì„ ìˆ˜ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    diffs = []
    shot_boundaries = [0]
    recent_diffs_window = []

    ret, prev_frame = cap.read()
    if not ret:
        print("Cannot read video's first frame")
        cap.release()
        return [], []

    if resize_dim:
        prev_frame = cv2.resize(prev_frame, resize_dim)

    print(f"'{video_label}' ë¹„ë””ì˜¤ ìƒ· ê²½ê³„ ê°ì§€ ì‹œì‘...")

    if resize_dim:
        print(f"í”„ë ˆì„ í¬ê¸° ì¡°ì •: {resize_dim[0]}x{resize_dim[1]}")

    # tqdm ì‚¬ìš© ì—¬ë¶€: í”„ë ˆì„ ìˆ˜ ëª¨ë¥´ë©´ progress bar ì—†ì´
    use_tqdm = total_frames > 0
    progress = tqdm(
        total=(total_frames - 1) if use_tqdm else None, desc="í”„ë ˆì„ ì²˜ë¦¬ ì¤‘"
    )

    frame_idx = 1
    while True:
        ret, current_frame = cap.read()
        if not ret:
            break

        if resize_dim:
            current_frame = cv2.resize(current_frame, resize_dim)

        diff = compute_hist_diff(prev_frame, current_frame, bins)
        diffs.append(diff)

        recent_diffs_window.append(diff)
        if len(recent_diffs_window) > window_size:
            recent_diffs_window.pop(0)

        threshold_value = 0.0
        is_shot_boundary = False

        if len(recent_diffs_window) >= 5:
            avg = np.mean(recent_diffs_window)
            std = np.std(recent_diffs_window)
            threshold_value = avg + scale_factor * std

            is_shot_boundary = diff > threshold_value
            if is_shot_boundary:
                shot_boundaries.append(frame_idx)

        if use_tqdm:
            progress.set_postfix(
                {
                    "í”„ë ˆì„": frame_idx,
                    "ì°¨ì´": f"{diff:.4f}",
                    "ì„ê³„ê°’": (
                        f"{threshold_value:.4f}"
                        if len(recent_diffs_window) >= 5
                        else "ê³„ì‚° ì¤‘"
                    ),
                    "ìƒ· ê²½ê³„": "O" if is_shot_boundary else "X",
                }
            )
            progress.update(1)

        prev_frame = current_frame
        frame_idx += 1

    if use_tqdm:
        progress.close()

    cap.release()
    print(f"\nìƒ· ê²½ê³„ ê°ì§€ ì™„ë£Œ. ì´ {len(shot_boundaries)}ê°œì˜ ìƒ· ê²½ê³„ ê°ì§€ë¨.")

    return shot_boundaries, diffs


def sum_short_boundaries(boundaries, max_gap=15):
    """
    ë„ˆë¬´ ì§§ì€ ìƒ·ë“¤ì„ ë³‘í•©í•˜ëŠ” í•¨ìˆ˜

    :param boundaries: ê¸°ì¡´ì— ë§Œë“¤ì–´ì§„ shot boundariesì˜ í”„ë ˆì„ ë²ˆí˜¸
    :param max_gap: max_gap ì´í•˜ì˜ shotë“¤ì€ í•˜ë‚˜ë¡œ í•©ì¹¨
    :return: ì§§ì€ ìƒ·ë“¤ì´ ì œê±°ëœ shot boundaries
    """

    representatives = []
    cur_group_start = boundaries[0]

    for i in range(1, len(boundaries)):
        if boundaries[i] - cur_group_start > max_gap:
            representatives.append(cur_group_start)
            cur_group_start = boundaries[i]

    representatives.append(cur_group_start)
    return representatives


def visualize_shot_detection(
    diffs,
    shot_boundaries,
    video_fps,
    total_video_frames,
    title="Hist Diff & Shot Boundary per Frame",
):
    """
    í”„ë ˆì„ ê°„ íˆìŠ¤í† ê·¸ë¨ ì°¨ì´(diffs)ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ê³ ,
    ê°ì§€ëœ ìƒ· ê²½ê³„ ì§€ì ì— ìƒ· ë²ˆí˜¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. Xì¶•ì€ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.

    :param diffs: ê° í”„ë ˆì„ ê°„ì˜ íˆìŠ¤í† ê·¸ë¨ ì°¨ì´ ê°’ ë¦¬ìŠ¤íŠ¸
    :param shot_boundaries: ìƒ· ê²½ê³„ í”„ë ˆì„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    :param video_fps: ë¹„ë””ì˜¤ì˜ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜ (Frame Per Second)
    :param total_video_frames: ë¹„ë””ì˜¤ì˜ ì´ í”„ë ˆì„ ìˆ˜
    :param title: ê·¸ë˜í”„ ì œëª©
    """
    plt.figure(figsize=(18, 6))  # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •

    # í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ ë³€í™˜
    time_indices_sec = np.arange(1, len(diffs) + 1) / video_fps

    plt.plot(
        time_indices_sec, diffs, label="Hist diff per Frame", color="blue", alpha=0.7
    )

    # ìƒ· ê²½ê³„ ì§€ì ì— ìƒ· ë²ˆí˜¸ í…ìŠ¤íŠ¸ í‘œì‹œ
    for i, boundary_frame in enumerate(shot_boundaries):
        if i > 0:  # ìƒ· 2ë¶€í„°
            shot_number = i + 1
            # í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•  x ì¢Œí‘œë¥¼ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ ë³€í™˜
            text_x = boundary_frame / video_fps
            text_y = np.max(diffs) * 0.95  # diffs ìµœëŒ€ê°’ì˜ 95% ìœ„ì¹˜ (ì¡°ì ˆ ê°€ëŠ¥)

            plt.text(
                text_x,
                text_y,
                str(shot_number),  # ìƒ· ë²ˆí˜¸ í…ìŠ¤íŠ¸
                color="red",
                fontsize=10,  # í°íŠ¸ í¬ê¸° 10ìœ¼ë¡œ ë³€ê²½ (ë” ì˜ ë³´ì´ë„ë¡)
                fontweight="bold",
                ha="center",  # ìˆ˜í‰ ì •ë ¬: ì¤‘ì•™
                va="top",  # ìˆ˜ì§ ì •ë ¬: ìœ„ìª½ (yê°’ì´ í…ìŠ¤íŠ¸ì˜ ìƒë‹¨ì— ì˜¤ë„ë¡)
                bbox=dict(
                    facecolor="yellow",
                    alpha=0.5,
                    edgecolor="none",
                    boxstyle="round,pad=0.2",
                ),  # ë°°ê²½ ë°•ìŠ¤
            )

    plt.title(title, fontsize=16)
    plt.xlabel("ì‹œê°„ (ì´ˆ)", fontsize=12)  # Xì¶• ë ˆì´ë¸” ë³€ê²½
    plt.ylabel("Histogram diffs (Bhattacharyya Distance)", fontsize=12)
    plt.grid(True, linestyle=":", alpha=0.6)
    plt.legend()
    # xì¶• ë²”ìœ„ë¥¼ ì´ ë¹„ë””ì˜¤ ì‹œê°„(ì´ˆ)ê¹Œì§€ ì„¤ì •
    plt.xlim(0, total_video_frames / video_fps)
    plt.tight_layout()  # ë ˆì´ì•„ì›ƒ ìë™ ì¡°ì •
    plt.show()


def create_segmented_video(input_video_path, shot_boundaries, output_filename):
    """
    ì£¼ì–´ì§„ ìƒ· ê²½ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ ë¹„ë””ì˜¤ë¥¼ ë¶„í• í•˜ê³ , ê° ìƒ· ê²½ê³„ì— ì‹œê°ì  í‘œì‹œë¥¼ ì¶”ê°€í•˜ì—¬
    í•˜ë‚˜ì˜ ìƒˆë¡œìš´ MP4 ë¹„ë””ì˜¤ íŒŒì¼ë¡œ í†µí•©

    :param input_video_path: ì›ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    :param shot_boundaries: ìƒ· ê²½ê³„ í”„ë ˆì„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ (shot_boundary_detection í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ê°’)
                            ì²« í”„ë ˆì„(0)ì€ í•­ìƒ ìƒ·ì˜ ì‹œì‘ìœ¼ë¡œ ê°„ì£¼
    :param output_filename: ìƒì„±ë  ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ì´ë¦„
    """

    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print(f"Cannot open video: {input_video_path}")
        return

    # ë¹„ë””ì˜¤ ì†ì„±
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(
        cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
    )
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    # ì½”ë± ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))

    print(f"'{input_video_path}'ì—ì„œ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
    print(f"ì¶œë ¥ íŒŒì¼: '{output_filename}'")

    shot_boundaries_set = set(shot_boundaries)

    current_shot_idx = 1
    current_shot_text = f"SHOT: {current_shot_idx}"

    # ê° ìƒ·ë§ˆë‹¤ ëœë¤ ìƒ‰ìƒ ìƒì„± (BGR í˜•ì‹)
    current_shot_color = np.random.randint(0, 256, 3).tolist()

    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 1.5
    font_thickness = 3

    with tqdm(total=total_frames, desc="í”„ë ˆì„ ì²˜ë¦¬ ë° ë¹„ë””ì˜¤ ìƒì„± ì¤‘") as pbar:
        for frame_idx in range(total_frames):
            ret, frame = cap.read()
            if not ret:
                break

            # í˜„ì¬ í”„ë ˆì„ì´ ìƒ· ê²½ê³„ì— í•´ë‹¹í•˜ëŠ”ì§€ í™•ì¸
            # ì²« í”„ë ˆì„(0)ì€ ì´ë¯¸ ì´ˆê¸°í™” ì‹œ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ, 0ì´ ì•„ë‹Œ ìƒ· ê²½ê³„ë§Œ ìƒˆë¡œìš´ ìƒ·ìœ¼ë¡œ ê°„ì£¼
            if frame_idx > 0 and frame_idx in shot_boundaries_set:
                current_shot_idx += 1  # ìƒˆë¡œìš´ ìƒ·ì´ ì‹œì‘ë˜ì—ˆìœ¼ë¯€ë¡œ ìƒ· ë²ˆí˜¸ ì¦ê°€
                current_shot_text = f"SHOT: {current_shot_idx}"
                current_shot_color = np.random.randint(
                    0, 256, 3
                ).tolist()  # ìƒˆë¡œìš´ ëœë¤ ìƒ‰ìƒ ìƒì„±

            # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚° (ë§¤ í”„ë ˆì„ë§ˆë‹¤ í…ìŠ¤íŠ¸ê°€ ë‹¬ë¼ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ í•œ ë²ˆë§Œ ê³„ì‚°í•´ë„ ë˜ì§€ë§Œ,
            # ì—¬ê¸°ì„œëŠ” í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚°ì„ ìœ„í•´ ë£¨í”„ ì•ˆì— ë‘¡ë‹ˆë‹¤.)
            (text_width, text_height), baseline = cv2.getTextSize(
                current_shot_text, font, font_scale, font_thickness
            )

            # í…ìŠ¤íŠ¸ ìœ„ì¹˜ (í”„ë ˆì„ ì¤‘ì•™ ìƒë‹¨)
            text_x = (width - text_width) // 2
            text_y = text_height + 20  # ìƒë‹¨ì—ì„œ 20í”½ì…€ ì•„ë˜

            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (í˜„ì¬ ìƒ·ì˜ í…ìŠ¤íŠ¸ì™€ ìƒ‰ìƒ ì‚¬ìš©)
            cv2.putText(
                frame,
                current_shot_text,
                (text_x, text_y),
                font,
                font_scale,
                current_shot_color,
                font_thickness,
                cv2.LINE_AA,
            )

            out.write(frame)  # ì²˜ë¦¬ëœ í”„ë ˆì„ì„ ì¶œë ¥ ë¹„ë””ì˜¤ì— ì“°ê¸°
            pbar.update(1)  # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸

    cap.release()  # ì…ë ¥ ë¹„ë””ì˜¤ ê°ì²´ í•´ì œ
    out.release()  # ì¶œë ¥ ë¹„ë””ì˜¤ ê°ì²´ í•´ì œ
    print(f"\në¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: '{output_filename}'")


# * ------------------- SI, TI, Optical Flow -------------------
def extract_frame_number(path):
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ì •ìˆ˜í˜• í”„ë ˆì„ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: '0s_30.0fps_896.jpg' â†’ 896)
    """
    match = re.search(r"(\d+)\.jpg", path)
    return int(match.group(1)) if match else -1


def calculateSI(path):
    """
    Spatial Information (SI) ê³„ì‚°\n
    SIëŠ” ì˜ìƒì˜ ë³µì¡ì„±ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ, Sobel í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒì˜ ì—£ì§€ ê°•ë„ë¥¼ ê³„ì‚°í•¨\n
    extractFrames í•¨ìˆ˜ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œ í›„ ì‚¬ìš© ê¶Œì¥

    @param path: ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
    """
    si_values = []
    frames = sorted(glob.glob(os.path.join(path, "*.jpg")), key=extract_frame_number)
    for img_path in tqdm(frames, desc="Calculating SI"):
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            continue
        sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)
        sobel = np.hypot(sobel_x, sobel_y)  # sqrt(sobel_x**2 + sobel_y**2)
        std = np.std(sobel)
        si_values.append(std)

    # return max(si_values) if si_values else 0.0
    return si_values


def calculateTI(path):
    """
    Temporal Information (TI) ê³„ì‚°\n
    TIëŠ” ì˜ìƒì˜ ì‹œê°„ì  ë³€í™”ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œë¡œ, ì—°ì†ëœ í”„ë ˆì„ ê°„ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•¨\n
    extractFrames í•¨ìˆ˜ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œ í›„ ì‚¬ìš© ê¶Œì¥

    @param path: ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
    """
    ti_values = []
    prev_img = None
    frames = sorted(glob.glob(os.path.join(path, "*.jpg")), key=extract_frame_number)

    for img_path in tqdm(frames, desc="Calculating TI"):
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            continue
        if prev_img is not None:
            diff = cv2.absdiff(img, prev_img)
            std = np.std(diff)
            ti_values.append(std)

        prev_img = img

    # return max(ti_values) if ti_values else 0.0
    return ti_values


def calculateOpticalFlow(path):
    """
    Optical Flowë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ ê°„ì˜ ì›€ì§ì„ì„ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜\n
    Optical FlowëŠ” ì—°ì†ëœ í”„ë ˆì„ ê°„ì˜ í”½ì…€ ì´ë™ì„ ê³„ì‚°í•˜ì—¬ ì˜ìƒì˜ ì›€ì§ì„ì„ ë¶„ì„í•¨\n
    extractFrames í•¨ìˆ˜ë¡œ í”„ë ˆì„ì„ ì¶”ì¶œ í›„ ì‚¬ìš© ê¶Œì¥

    @param path: ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
    """
    frames = sorted(glob.glob(os.path.join(path, "*.jpg")), key=extract_frame_number)
    flow_magnitudes = []

    prev = None
    prev_pts = None
    imputed_indices = []  # ë³´ê°„ëœ í”„ë ˆì„ ì¸ë±ìŠ¤ ê¸°ë¡

    for i, img_path in enumerate(tqdm(frames, desc="Calculating Optical Flow")):
        frame = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if frame is None:
            continue

        if prev is not None and prev_pts is not None:
            next_pts, status, _ = cv2.calcOpticalFlowPyrLK(prev, frame, prev_pts, None)

            if next_pts is not None and status is not None:
                good_prev = prev_pts[status.flatten() == 1]
                good_next = next_pts[status.flatten() == 1]

                if len(good_prev) > 0:
                    displacements = np.linalg.norm(good_next - good_prev, axis=1)
                    mean_disp = np.mean(displacements)
                    flow_magnitudes.append(mean_disp)
                else:
                    # ì¶”ì  ì‹¤íŒ¨ â†’ ì´ì „ ê°’ ë³´ê°„
                    imputed_indices.append(i)
                    flow_magnitudes.append(
                        flow_magnitudes[-1] if flow_magnitudes else 0
                    )
            else:
                # Optical flow ê³„ì‚° ì‹¤íŒ¨ â†’ ë³´ê°„
                imputed_indices.append(i)
                flow_magnitudes.append(flow_magnitudes[-1] if flow_magnitudes else 0)
        elif prev is not None:
            # feature ì¶”ì¶œ ì‹¤íŒ¨ â†’ ë³´ê°„
            imputed_indices.append(i)
            flow_magnitudes.append(flow_magnitudes[-1] if flow_magnitudes else 0)

        prev = frame
        prev_pts = cv2.goodFeaturesToTrack(
            frame, maxCorners=100, qualityLevel=0.3, minDistance=7
        )

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    if imputed_indices:
        print(f"[Optical Flow] ë³´ê°„ëœ í”„ë ˆì„ ìˆ˜: {len(imputed_indices)}ê°œ")
        print(f"[Optical Flow] ë³´ê°„ëœ í”„ë ˆì„ ì¸ë±ìŠ¤: {imputed_indices}")
    else:
        print("[Optical Flow] ëª¨ë“  í”„ë ˆì„ì—ì„œ ì •ìƒì ìœ¼ë¡œ Optical Flow ê³„ì‚° ì™„ë£Œ.")

    return flow_magnitudes


# * ------------------- Audioë¡œ SBD -------------------


def get_topk_indices_from_whole_audio(audio_np, device="cuda", top_k=5):
    """
    ì „ì²´ ì˜¤ë””ì˜¤ì—ì„œ ìƒìœ„ K ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜

    @param audio_np: ì˜¤ë””ì˜¤ ë°ì´í„° (1, N) numpy array
    @param device: 'cpu' ë˜ëŠ” 'cuda'
    @param top_k: ìƒìœ„ K ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜´
    @return: ìƒìœ„ K ê°œì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    at = AudioTagging(checkpoint_path=None, device=device)
    (clipwise_output, _) = at.inference(torch.tensor(audio_np, device=device).float())
    topk_indices = np.argsort(clipwise_output[0])[::-1][:top_k]
    return topk_indices


def extractPANNsVectors(audio_np, sr=32000, segment_sec=1.0, device="cuda", top_k=5):
    """
    [1, N] shaped numpy ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ PANNs ë²¡í„° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n
    top_kê°€ ì „ì²´ ì˜ìƒì— ëŒ€í•œ topì´ ì•„ë‹Œ, ê° segmeentì— ëŒ€í•œ top_kì„

    @param audio_np: ì˜¤ë””ì˜¤ ë°ì´í„° (1, N) numpy array
    @param sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 32000)
    @param segment_sec: ê° segmentì˜ ê¸¸ì´ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0ì´ˆ)
    @param device: 'cpu' ë˜ëŠ” 'cuda'
    @param top_k: ê° segmentì—ì„œ ì¶”ì¶œí•  ìƒìœ„ K ê°œ í´ë˜ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: 5)
    @return: PANNs ë²¡í„° ì‹œí€€ìŠ¤ (shape: [N, top_k]), segment_sec
    """
    audio_tensor = torch.tensor(audio_np, device=device).float()  # shape: [1, N]

    chunk_size = int(sr * segment_sec)
    total_len = audio_tensor.shape[1]
    total_chunks = total_len // chunk_size

    at = AudioTagging(checkpoint_path=None, device=device)
    vector_list = []

    for i in tqdm(range(total_chunks), desc="PANNs ë²¡í„° ì¶”ì¶œ ì¤‘"):
        chunk = audio_tensor[:, i * chunk_size : (i + 1) * chunk_size]
        if chunk.shape[1] != chunk_size:
            continue

        (clipwise_output, _) = at.inference(chunk)
        scores = torch.tensor(
            clipwise_output[0], device=device
        ).float()  # shape: (527,)
        vector_list.append(scores)

    if not vector_list:
        return None, segment_sec

    all_vectors = torch.stack(vector_list, dim=0)  # shape: (N, 527)

    # ğŸ¯ í‰ê·  ê¸°ì¤€ top-k í´ë˜ìŠ¤ ì„ íƒ
    mean_scores = torch.mean(all_vectors, dim=0)
    topk_indices = torch.topk(mean_scores, k=top_k).indices  # shape: (top_k,)

    vectors_topk = all_vectors[:, topk_indices]  # shape: (N, top_k)

    return vectors_topk, segment_sec


def extractPANNsVectorsTopK(
    audio_np, topk_indices, sr=32000, segment_sec=1.0, device="cuda"
):
    """
    [1, N] shaped numpy ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ PANNs ë²¡í„° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n
    top_kê°€ ì „ì²´ ì˜ìƒì— ëŒ€í•œ top_kì„

    @param audio_np: ì˜¤ë””ì˜¤ ë°ì´í„° (1, N) numpy array
    @param topk_indices: PANNs ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìƒìœ„ K ê°œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    @param sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 32000)
    @param segment_sec: ê° segmentì˜ ê¸¸ì´ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0ì´ˆ)
    @param device: 'cpu' ë˜ëŠ” 'cuda'
    @return: PANNs ë²¡í„° ì‹œí€€ìŠ¤ (shape: [N, top_k]), segment_sec
    """
    audio_tensor = torch.tensor(audio_np, device=device).float()  # shape: [1, N]

    chunk_size = int(sr * segment_sec)
    total_len = audio_tensor.shape[1]
    total_chunks = total_len // chunk_size

    at = AudioTagging(checkpoint_path=None, device=device)
    vector_list = []

    for i in tqdm(range(total_chunks), desc="PANNs ë²¡í„° ì¶”ì¶œ ì¤‘"):
        chunk = audio_tensor[:, i * chunk_size : (i + 1) * chunk_size]
        if chunk.shape[1] != chunk_size:
            continue

        (clipwise_output, _) = at.inference(chunk)
        scores = clipwise_output[0][topk_indices]  # shape: (top_k,)
        scores_tensor = torch.tensor(scores, device=device).float()
        vector_list.append(scores_tensor)

    if not vector_list:
        return None, segment_sec

    return torch.stack(vector_list, dim=0), segment_sec


def detectAudioShotBoundaries(
    vectors, threshold=None, scale=1.0, return_distances=False
):
    """
    cosine distance ê¸°ë°˜ ìƒ· ê²½ê³„ ê²€ì¶œ (GPU ë²¡í„° ì…ë ¥)

    @param vectors: shape (T, top_k) PANNs ë²¡í„° ì‹œí€€ìŠ¤
    @param threshold: ìƒ· ê²½ê³„ ì„ê³„ê°’, Noneì´ë©´ ìë™ ê³„ì‚°
    @param scale: thresholdê°€ Noneì¼ ë•Œ, ìë™ threshold = mean + scale * std
    @param return_distances: ë””ë²„ê¹…ìš©, distance ë²¡í„° ë°˜í™˜ ì—¬ë¶€
    @return: ìƒ· ê²½ê³„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸, distances (ì„ íƒì )
    """
    vectors = F.normalize(vectors, dim=1)
    similarities = torch.sum(vectors[1:] * vectors[:-1], dim=1)
    distances = 1 - similarities

    if threshold is None:
        mean = distances.mean().item()
        std = distances.std().item()
        threshold = mean + scale * std
        print(
            f"[AUTO] Threshold = Mean({mean:.4f}) + {scale:.2f} * Std({std:.4f}) = {threshold:.4f}"
        )

    boundary_indices = torch.where(distances > threshold)[0] + 1
    boundaries = [0] + boundary_indices.tolist()

    if return_distances:
        return boundaries, distances.cpu().numpy()

    return boundaries


def visualizeAudioVectors(vectors, boundaries, topk_indices, segment_sec=1.0):
    """
    PANNs ë²¡í„° ì‹œí€€ìŠ¤ì™€ ìƒ· ê²½ê³„ ì‹œì ì„ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”

    @param vectors: shape (T, top_k) PANNs ë²¡í„° ì‹œí€€ìŠ¤
    @param boundaries: ìƒ· ê²½ê³„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    @param topk_indices: ì›ë˜ 527ì°¨ì› ì¤‘ ì–´ë–¤ indexë¥¼ top-kë¡œ ì¼ëŠ”ì§€
    @param segment_sec: ê° segmentì˜ ê¸¸ì´ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1.0ì´ˆ)
    @return: ì—†ìŒ (ê·¸ë˜í”„ ì¶œë ¥)
    """

    # torch â†’ numpy ë³€í™˜
    vectors = vectors.detach().cpu().numpy()

    # label ì´ë¦„ ì¶”ì¶œ
    selected_labels = [labels[i] for i in topk_indices]

    # ì‹œê°í™” ì‹œì‘
    plt.figure(figsize=(14, 5))
    sns.heatmap(vectors.T, cmap="magma", xticklabels=False, yticklabels=selected_labels)
    plt.title("Top Audio Event Scores Over Time")
    plt.ylabel("Top Audio Classes")
    plt.xlabel("Time (seconds, per segment)")

    # ìƒ· ê²½ê³„ ì‹œì  ì„  ê¸‹ê¸°
    for t in boundaries:
        plt.axvline(x=t / segment_sec, color="cyan", linestyle="--", linewidth=1)

    plt.tight_layout()
    plt.show()
